{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import (cross_validate, cross_val_predict,cross_val_score, train_test_split, StratifiedKFold, \n",
    "                                    permutation_test_score, GridSearchCV, RandomizedSearchCV, RandomizedSearchCV)\n",
    "from pprint import pprint\n",
    "from joblib import dump, load\n",
    "import joblib\n",
    "import pickle\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_feat_importance(importance, feature_names):\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(feature_names)\n",
    "\n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "    fi_df['cum_sum_fi']=fi_df['feature_importance'].cumsum()\n",
    "    return fi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "### 1. Load Data\n",
    "- Questions Data\n",
    "- Feature Data\n",
    "### 2. Data Preprocessing\n",
    "- Hyperparametertuning: Optimize Ratio: (Nr of feature)/(Train set size)\n",
    "- Downsampling the dataset with chosen features\n",
    "- Visualization of the feature distributions\n",
    "- Feature Encoding: Label Encoding\n",
    "### 3. Baseline Model: Naive Bayes\n",
    "### 4. Random Forest Model\n",
    "- Hyperparametertuning with RandomSearch\n",
    "- Feature importance\n",
    "### 5. Evaluation \n",
    "- Permutation Test\n",
    "- Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv('data/questions_preprocessed.csv', sep=';', index_col=0)\n",
    "descriptive_questions = [i for i in questions.index if i.startswith('dq')]\n",
    "questions.loc[descriptive_questions, 'Order'] = questions.loc[descriptive_questions, 'Order'].apply(lambda row: ast.literal_eval(row))\n",
    "\n",
    "# drop descriptive questions that reference politics \n",
    "for i in descriptive_questions:\n",
    "    if any(keyword in questions.loc[i, 'text'] for keyword in ['politics', 'political', 'politician']):\n",
    "            descriptive_questions.remove(i)\n",
    "            \n",
    "display(questions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('data/train.parquet')\n",
    "# per question count number of times the question is answered\n",
    "n_answers_per_question = data.loc[:, descriptive_questions].count(axis=0)\n",
    "sorted_questions = n_answers_per_question.sort_values(ascending=False).index\n",
    "\n",
    "question_data = questions.join(n_answers_per_question.to_frame('n_answers')) \n",
    "display(questions.loc[sorted_questions[:10]][['text']])\n",
    "\n",
    "features_df = data[descriptive_questions]\n",
    "print('Nr of rows, nr of descriptive questions', features_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relative frequencies\n",
    "freqs = data['political_belief'].value_counts(normalize=True)*100\n",
    "title = questions.loc['q212813', 'text']\n",
    "freqs = freqs.rename(index = {'Liberal / Left-wing': 'Liberal/Left-wing', 'Conservative / Right-wing': 'Conservative/Right-wing'})\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor('white')\n",
    "plt.bar(freqs.index, \n",
    "            freqs.values,\n",
    "            width=0.7,\n",
    "            tick_label=['Liberal', 'Centrist', 'Conservative', 'Other'],\n",
    "            color=['#5770db', '#db57b2', '#ba2832', '#fdbf6f'])\n",
    "plt.title(title)\n",
    "plt.ylabel('percentage (%)')\n",
    "plt.xlabel(\"Political Beliefs\")\n",
    "plt.tick_params(bottom=True, left=True)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['bottom'].set_color('black')\n",
    "ax.spines['left'].set_visible(True)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.spines['left'].set_color('black')\n",
    "ax.axhline(y=6.882451, color='gray', linestyle='-', label='Downsampling threshold')\n",
    "plt.legend(frameon=False) \n",
    "plt.savefig(\"data/images/Political_Belief_Distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn, df_val = train_test_split(data[descriptive_questions + ['political_belief', 'gender', 'd_age']], test_size=0.1, random_state=1)\n",
    "df_val = df_val[descriptive_questions+['political_belief']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "To keep the train set reasonably big, we compute, how many questions we can include, s.t. that the nr. of subjects who answered these questions is at least 8.000. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=200\n",
    "subset_size_topk = [features_df.loc[:, sorted_questions[:k]].dropna().shape[0] for k in range(1, K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biggest k with >8.000 to 25.000 subjects who answered ALL k questions\n",
    "min_train_size = np.arange(8, 25)*1000\n",
    "n_most_answered = [next(x[0] for x in enumerate(subset_size_topk) if x[1] < min_set_size) for min_set_size in min_train_size]\n",
    "n_most_answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of validation set before: {df_val.shape}')\n",
    "top_questions = sorted_questions[:np.max(n_most_answered)].to_list()\n",
    "df_val_topq = df_val[top_questions+['political_belief']].dropna() \n",
    "print(f'Shape of validation set after selecting the top {np.max(n_most_answered)} questions: {df_val_topq.shape}')\n",
    "\n",
    "X_val_topq = df_val_topq[top_questions]\n",
    "y_val = df_val_topq[['political_belief']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, min_size in zip(n_most_answered[:2], min_train_size):\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_style(\"ticks\")\n",
    "    sns.set\n",
    "    g = sns.lineplot(x = range(1,K), y = subset_size_topk, linewidth=1)\n",
    "    g.axhline(y=min_size, color='red', linewidth=0.5)\n",
    "    g.axvline(x=n, color='red', linewidth=0.5)\n",
    "\n",
    "    plt.xlim(0, 200)\n",
    "    plt.ylim(0)\n",
    "    sns.despine()\n",
    "    #plt.ylim(0,len(features_df))\n",
    "    g.set(xlabel =\"Cardinality of set of most answered questions\", ylabel = \"Nr of subjects who answered the questions\")\n",
    "    #title ='Nr of subjects who answered the same k most anwered questions '\n",
    "    #plt.figtext(0.5, -0.1, 'Grouping most answered questions into a set, comparing the number of questions in the set to the number of subjects who answered all questions in the set', wrap=True, horizontalalignment='center', fontsize=12)\n",
    "    plt.title(f'minimum train size before downsampling: {min_size}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO: Optimize Ratio: (Nr of feature)/(Train set size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = []\n",
    "\n",
    "for n in n_most_answered:\n",
    "    top_questions = sorted_questions[:n].to_list()\n",
    "    filter_by = top_questions + ['political_belief']\n",
    "    downsampled_df = df_trn[filter_by].dropna()\n",
    "    n_conservative = downsampled_df.groupby('political_belief').count().iloc[1, 0]\n",
    "    downsampled_df = downsampled_df.groupby('political_belief', group_keys=False).apply(lambda x: x.sample(n=n_conservative, random_state=1))\n",
    "\n",
    "\n",
    "    # split into features and target\n",
    "    features_df_downsampled = downsampled_df[top_questions]\n",
    "    target = downsampled_df.loc[features_df_downsampled.index][['political_belief']]\n",
    "\n",
    "    # encode train and validation set\n",
    "    feature_encoder = OrdinalEncoder()\n",
    "    target_encoder = LabelEncoder()\n",
    "    # train set\n",
    "    X_train_encoded = feature_encoder.fit_transform(features_df_downsampled)\n",
    "    y_train_encoded = target_encoder.fit_transform(target.values.ravel())\n",
    "    # val set\n",
    "    X_val_encoded = feature_encoder.transform(X_val_topq[top_questions])\n",
    "    y_val_encoded = target_encoder.transform(y_val.values.ravel())\n",
    "\n",
    "    # Naive Bayes Hyper parameter Tuning\n",
    "    nb = CategoricalNB(fit_prior=False).fit(X_train_encoded, y_train_encoded)\n",
    "    preds = nb.predict(X_val_encoded)\n",
    "    val_scores.append(balanced_accuracy_score(y_val_encoded, preds))\n",
    "\n",
    "best_nr_of_questions = n_most_answered[np.argmax(val_scores)]\n",
    "print(f'Best balanced accuracy on the validation set obtained with {best_nr_of_questions} number of answered questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\")\n",
    "g = sns.lineplot(x = n_most_answered, y = val_scores, linewidth=1)\n",
    "g.axhline(y=np.max(val_scores), color='red', linewidth=0.5)\n",
    "g.axvline(x=best_nr_of_questions, color='red', linewidth=0.5)\n",
    "sns.despine()\n",
    "g.set(ylabel ='Balanced Accuracy Score', xlabel ='Nr of questions as features', title='Evaluation on validation set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling the dataset with chosen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nr_of_questions = 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_questions = sorted_questions[:best_nr_of_questions].to_list()\n",
    "filter_by = top_questions + ['political_belief', 'd_age', 'gender']\n",
    "downsampled_df = data[filter_by].dropna()\n",
    "n_conservative = downsampled_df.groupby('political_belief').count().iloc[1, 0]\n",
    "downsampled_df = downsampled_df.groupby('political_belief', group_keys=False).apply(lambda x: x.sample(n=n_conservative, random_state=1))\n",
    "display(downsampled_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize of the feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_question_density(question: str, data: pd.DataFrame, questions: pd.DataFrame, save:bool=False, x_lim=None, legend:bool=True, font_scale=1.5):\n",
    "    x,y = question, 'political_belief'\n",
    "    df1 = ((data.groupby(x)[y].value_counts())/len(data)).rename('percentage').reset_index()\n",
    "    df1['percentage'] = df1['percentage']*100\n",
    "    title = questions.loc[question, 'text']\n",
    "    plt.figure(figsize=(8,20))\n",
    "    sns.set(font_scale=font_scale, font='Times New Roman')\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_style(\"ticks\")\n",
    "    if not x_lim:\n",
    "        x_lim = np.max(df1['percentage'])+2\n",
    "    g = sns.catplot(x='percentage',y=x,hue=y, kind='bar', data=df1, \n",
    "                hue_order= ['Liberal / Left-wing', 'Centrist', 'Conservative / Right-wing', 'Other'], \n",
    "                palette={'Liberal / Left-wing': '#5770db', 'Centrist': '#db57b2', 'Conservative / Right-wing': '#ba2832', 'Other': '#fdbf6f'},\n",
    "                order = questions.loc[question, 'Order'],\n",
    "                legend=False,\n",
    "                height = 10, aspect = 3,\n",
    "    \n",
    "                ).set(xlabel =\"percentage (%)\", ylabel = \"\", title=title) \n",
    "    #plt.xlim(0,x_lim)\n",
    "    plt.xticks(np.arange(0, x_lim+1, 2))     \n",
    "    if legend:\n",
    "        plt.legend(title=\"\", bbox_to_anchor=(1, 1), loc='upper left', fontsize=40) \n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(f'data/images/{question}.png',bbox_inches='tight')\n",
    "    sns.despine(right=True, top=True)\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2_important = ['dq586', 'dq9']\n",
    "plot_question_density('dq586', downsampled_df, questions, x_lim=16, save=True, legend=True, font_scale=4)\n",
    "plot_question_density('dq9', downsampled_df, questions, x_lim=16, save=True, font_scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into features and target\n",
    "features_df_topq = downsampled_df[top_questions]\n",
    "target = downsampled_df.loc[features_df_topq.index][['political_belief']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_encoder = OrdinalEncoder()\n",
    "target_encoder = LabelEncoder()\n",
    "X_train_encoded = feature_encoder.fit_transform(features_df_topq)\n",
    "y_train_encoded = target_encoder.fit_transform(target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(feature_encoder, open('models/feature_encoder.pkl', 'wb'))\n",
    "pickle.dump(target_encoder, open('models/target_encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of final downsampled trainset')\n",
    "X_train_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = CategoricalNB(fit_prior=False).fit(X_train_encoded, y_train_encoded)\n",
    "scores = cross_val_score(nb, X_train_encoded, y_train_encoded, cv=3, scoring='balanced_accuracy')\n",
    "print(f\"balanced accuracy with mean: {scores.mean()} a standard deviation of {scores.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(nb, 'models/nb_downsampled_trn_set.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "## Hyperparameter Tuning\n",
    "HPT using Random Search with downsampled K-fold crossvalidation \\\n",
    "Chosen scoring metrics: balanced accuracy (average of recall obtained on each class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose parameter distributions\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1500, num = 10)]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 15)]\n",
    "min_samples_split = [8, 10, 12, 14, 15, 16, 18]\n",
    "min_samples_leaf = [1, 2, 3, 4]\n",
    "bootstrap = [True, False]\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "              'bootstrap': bootstrap,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               }\n",
    "pprint(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(class_weight='balanced', random_state=1)\n",
    "rf_random = RandomizedSearchCV(estimator = rf,\n",
    "                             param_distributions = param_grid, \n",
    "                             n_iter = 500, \n",
    "                             cv = 3, \n",
    "                             verbose=2, \n",
    "                             random_state=42, \n",
    "                             n_jobs = -1, \n",
    "                             scoring='balanced_accuracy')\n",
    "rf_random.fit(X_train_encoded, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results of parameter tuning with random search:')\n",
    "best_random_params = rf_random.best_params_\n",
    "pprint(best_random_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned = rf_random.best_estimator_\n",
    "scores = cross_val_score(rf_tuned, X_train_encoded, y_train_encoded, cv=5, scoring='balanced_accuracy')\n",
    "print(f\"balanced accuracy with mean: {scores.mean()} a standard deviation of {scores.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "joblib.dump(rf_tuned, 'models/rf_downsampled_trn_set.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_df = get_sorted_feat_importance(rf_tuned.feature_importances_, features_df_topq.columns)\n",
    "print('Top 20 Features sorted by their gini impotance scores ')\n",
    "for i, row in fi_df[:20].iterrows():\n",
    "    print(f\"Feature {row['feature_names']}: {questions.loc[row['feature_names']].text}: {row['feature_importance']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define size of bar plot\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\")\n",
    "#Plot Searborn bar chart\n",
    "g = sns.barplot(y=fi_df['feature_importance'].values, x=np.arange(len(fi_df.index)), color='gray', alpha=0.7)\n",
    "sns.despine()\n",
    "\n",
    "g.set_xticklabels(fi_df['feature_names'])\n",
    "#Add chart labels\n",
    "g.set_title('Random forest: Sorted feature imporatance (gini)')\n",
    "g.set(ylabel ='feature impotance', xlabel ='question_ids')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first top k question, s.t. Cumultative sum of feature importance is >=.95\n",
    "cum_sum_top_k = next(x[0] for x in enumerate(fi_df.cum_sum_fi) if x[1] >=0.95)\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\")\n",
    "g = sns.lineplot(x = np.arange(len(fi_df)), y = fi_df['cum_sum_fi'])\n",
    "g.axhline(y=fi_df['cum_sum_fi'].iloc[cum_sum_top_k], color='red', linewidth=0.5)\n",
    "g.axvline(x=cum_sum_top_k, color='red', linewidth=0.5)\n",
    "sns.despine()\n",
    "plt.ylim(0)\n",
    "plt.xlim(0)\n",
    "g.set(xlabel='number of included features', ylabel='cum. score', title='Cumultative sum of feature importance', xticks=np.arange(len(fi_df), step=2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Test\n",
    "\n",
    "\n",
    "Reference:\n",
    "\n",
    "[1]\n",
    "Ojala and Garriga. Permutation Tests for Studying Classifier Performance. The Journal of Machine Learning Research (2010) vol. 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Permutation Test\n",
    "\n",
    "rf_tuned = joblib.load(open('models/rf_downsampled_trn_set.joblib', 'rb'))\n",
    "\n",
    "rng = np.random.RandomState(seed=0)\n",
    "# Use same number of samples and features as in the original data \n",
    "X_rand = np.random.permutation(X_train_encoded)\n",
    "\n",
    "clf = rf_tuned\n",
    "cv = StratifiedKFold(2, shuffle=True, random_state=1)\n",
    "\n",
    "score_data, perm_scores_data, pvalue_data = permutation_test_score(\n",
    "    clf, X_train_encoded, y_train_encoded, scoring=\"balanced_accuracy\", cv=cv, n_permutations=1000, n_jobs=-3\n",
    ")\n",
    "\n",
    "score_rand, perm_scores_rand, pvalue_rand = permutation_test_score(\n",
    "    clf, X_rand, y_train_encoded, scoring=\"balanced_accuracy\", cv=cv, n_permutations=1000, n_jobs = -3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "g = sns.histplot(perm_scores_data, binwidth=0.009, common_norm=True, label='scores on permuted data')\n",
    "plt.vlines(score_data, ymin=0, ymax=370, ls=\"--\", color=\"r\", label='score on original data')\n",
    "\n",
    "g.legend(title=f'p-value: {pvalue_data:.3f}',frameon=False)\n",
    "g.set_xlim(0,1)\n",
    "g.set_xlabel('Balanced accuracy score')\n",
    "g.set_xticks([0, perm_scores_data.mean(), score_data.round(4), 0.5, 0.75, 1])\n",
    "g.set_xticklabels([0, perm_scores_data.mean().round(2), score_data.round(4), 0.5, 0.75, 1])\n",
    "sns.despine()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "g = sns.histplot(perm_scores_rand, binwidth=0.009, common_norm=True, label='scores on permuted data')\n",
    "plt.vlines(score_rand, ymin=0, ymax=360, ls=\"--\", color=\"r\", label='score on random data')\n",
    "\n",
    "g.legend(title=f'p-value: {pvalue_rand:.2f}',frameon=False)\n",
    "g.set_xlim(0,1)\n",
    "g.set_xlabel('Balanced accuracy score')\n",
    "g.set_xticks([0, perm_scores_rand.mean(), score_rand.round(2), 0.5, 0.75, 1])\n",
    "g.set_xticklabels([0, perm_scores_rand.mean().round(2), score_rand.round(2), 0.5, 0.75, 1])\n",
    "sns.despine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "890d4ee240ac5ac260547f729684c54cbff557abe3fc52197092a1a13992c0b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
